import os, requests, time
from typing import Dict, Any
from bs4 import BeautifulSoup
from dotenv import load_dotenv
load_dotenv(".env")

class GoogleSearchTool:
    """–†–æ–∑—à–∏—Ä–µ–Ω–∏–π Google Search –∑ –∞–Ω–∞–ª—ñ–∑–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
    
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.search_engine_id = os.getenv("GOOGLE_SEARCH_ENGINE_ID")
        self.enabled = bool(self.api_key and self.search_engine_id)

    def search_with_analysis(self, query: str, num_results: int = 3) -> Dict[str, Any]:
        """–ü–æ—à—É–∫ –∑ Google —Ç–∞ –∞–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–æ–∫"""
        try:
            if not self.enabled:
                return {
                    'success': False,
                    'error': 'Google Search –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π'
                }
            
            # 1. –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É
            search_results = self._get_search_results(query, num_results)
            
            if not search_results['success']:
                return search_results
            
            # 2. –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            analyzed_sources = []
            
            for result in search_results['results'][:num_results]:
                
                content_analysis = self._analyze_page_content(
                    result['link'], 
                    result['title'],
                    result['snippet']
                )
                
                analyzed_sources.append({
                    'title': result['title'],
                    'url': result['link'],
                    'snippet': result['snippet'],
                    'content': content_analysis['content'],
                    'success': content_analysis['success'],
                    'word_count': content_analysis.get('word_count', 0)
                })
                
                # –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
                time.sleep(0.5)
            
            return {
                'success': True,
                'query': query,
                'sources': analyzed_sources,
                'total_found': len(analyzed_sources)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def _get_search_results(self, query: str, num_results: int = 3) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É –∑ Google API"""
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': self.api_key,
                'cx': self.search_engine_id,
                'q': query,
                'num': num_results,
                'hl': 'uk'  # –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if 'items' not in data:
                return {
                    'success': False,
                    'error': '–ù—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ'
                }
            
            results = []
            for item in data['items']:
                results.append({
                    'title': item.get('title', ''),
                    'link': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'displayLink': item.get('displayLink', '')
                })
            
            return {
                'success': True,
                'results': results,
                'total_results': data.get('searchInformation', {}).get('totalResults', 0)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Google API –ø–æ–º–∏–ª–∫–∞: {str(e)}'
            }
    
    def _analyze_page_content(self, url: str, title: str, snippet: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
        try:
            # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è headers –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–ª–æ–∫—É–≤–∞–Ω–Ω—è
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'uk-UA,uk;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –í–∏–¥–∞–ª—è—î–º–æ —Å–∫—Ä–∏–ø—Ç–∏ —Ç–∞ —Å—Ç–∏–ª—ñ
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç
            text = soup.get_text()
            
            # –û—á–∏—â—É—î–º–æ —Ç–µ–∫—Å—Ç
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —Ç–µ–∫—Å—Ç—É
            max_chars = 3000
            if len(text) > max_chars:
                text = text[:max_chars] + "..."
            
            return {
                'success': True,
                'content': text,
                'word_count': len(text.split()),
                'char_count': len(text)
            }
            
        except Exception as e:
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ snippet —è–∫ fallback
            return {
                'success': False,
                'content': snippet,
                'word_count': len(snippet.split()),
                'error': str(e)
            }
    
    def search(self, query: str) -> str:
        """–ó–≤–∏—á–∞–π–Ω–∏–π –ø–æ—à—É–∫ (–¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)"""
        result = self.search_with_analysis(query, 3)
        
        if not result['success']:
            return f"‚ùå –ü–æ–º–∏–ª–∫–∞ Google –ø–æ—à—É–∫—É: {result.get('error', '–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞')}"
        
        response = f"üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ Google –ø–æ—à—É–∫—É –¥–ª—è:** '{query}'\n\n"
        
        for i, source in enumerate(result['sources'], 1):
            response += f"**{i}. {source['title']}**\n"
            response += f"üîó {source['url']}\n"
            response += f"üìÑ {source['snippet']}\n\n"
        
        return response