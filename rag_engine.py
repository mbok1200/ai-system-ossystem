import json
import os
import logging
from typing import List, Dict
from openai import OpenAI
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer
from interfaces.dialogue_state import DialogueState
from tools.config.functions import get_functions
class RAGEngine:
    """–°–∏—Å—Ç–µ–º–∞ –ø–æ—à—É–∫—É —á–µ—Ä–µ–∑ Pinecone RAG"""
    
    def __init__(self, pinecone_index_name: str = "streamlit"):
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Pinecone (–Ω–æ–≤–∏–π API)
        self.pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    
        self.index_name = pinecone_index_name
        self.index = self.pc.Index(pinecone_index_name)
        self._detect_embedding_model()
        self._log_index_stats()
    
    def _detect_embedding_model(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ embedding"""
        try:
            stats = self.index.describe_index_stats()
            dimension = stats.dimension
            
            if dimension == 1536:
                self.embedding_model = "text-embedding-ada-002"
            elif dimension == 768:
                self.embedding_model = "local"  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ª–æ–∫–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å
            elif dimension == 3072:
                self.embedding_model = "text-embedding-3-large"
            else:
                self.embedding_model = "local"  # fallback
                
        except Exception as e:
            self.embedding_model = "local"
    
    def _log_index_stats(self):
        try:
            stats = self.index.describe_index_stats()
            if stats.namespaces:
                for ns, data in stats.namespaces.items():
                    ns_name = ns if ns else "''"
                    print(f"   Namespace {ns_name}: {data['vector_count']} –≤–µ–∫—Ç–æ—Ä—ñ–≤")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

    def search(self, query: str, top_k: int = 5) -> Dict:
        try:
            stats = self.index.describe_index_stats()
            total_vectors = stats.total_vector_count                   
            if total_vectors == 0:
                return {
                    'success': False,
                    'context': '',
                    'score': 0.0,
                    'sources': [],
                    'raw_results': [],
                    'message': '–ë–∞–∑–∞ –∑–Ω–∞–Ω—å –ø–æ—Ä–æ–∂–Ω—è'
                }
            
            # –ì–µ–Ω–µ—Ä—É—î–º–æ embedding –¥–ª—è –∑–∞–ø–∏—Ç—É
            embedding = self._get_embedding(query)
            
            # –®—É–∫–∞—î–º–æ –≤ default namespace (–¥–µ –±—ñ–ª—å—à–µ –≤–µ–∫—Ç–æ—Ä—ñ–≤)
            results = self.index.query(
                vector=embedding,
                top_k=top_k,
                include_metadata=True,
                namespace="default"  # ‚Üê –ö–ª—é—á–æ–≤–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è!
            )
                    
            # –Ø–∫—â–æ –≤ default –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤, —Å–ø—Ä–æ–±—É—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π namespace
            if len(results.matches) < top_k // 2:
                empty_results = self.index.query(
                    vector=embedding,
                    top_k=top_k,
                    include_metadata=True,
                    namespace=""  # –ü–æ—Ä–æ–∂–Ω—ñ–π namespace
                )                
                # –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
                all_matches = list(results.matches) + list(empty_results.matches)
                # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ score
                all_matches.sort(key=lambda x: x.score, reverse=True)
                results.matches = all_matches[:top_k]
            
            if not results.matches:
                return {
                    'success': False,
                    'context': '',
                    'score': 0.0,
                    'sources': [],
                    'raw_results': [],
                    'message': '–í –±–∞–∑—ñ –∑–Ω–∞–Ω—å –Ω—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ'
                }
            
            # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            all_results = []
            context_parts = []
            sources = []
            total_score = 0
            
            # –í—ñ–¥—Å–æ—Ä—Ç—É—î–º–æ –≤—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞ score (–≤—ñ–¥ –Ω–∞–π–±—ñ–ª—å—à–æ–≥–æ –¥–æ –Ω–∞–π–º–µ–Ω—à–æ–≥–æ)
            sorted_matches = sorted(results.matches, key=lambda x: x.score, reverse=True)

            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –≤—Å—ñ –∑ score > 0.73
            relevant_matches = [m for m in sorted_matches if m.score > 0.73]

            # –Ø–∫—â–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –º–µ–Ω—à–µ 2, –¥–æ–±–∏—Ä–∞—î–º–æ —â–µ –Ω–∞–π–±–ª–∏–∂—á—ñ (–Ω–∞–π–≤–∏—â—ñ –∑–∞ score)
            if len(relevant_matches) < 2:
                relevant_matches = sorted_matches[:2]

            for i, match in enumerate(relevant_matches, 1):
                result_info = {
                    'rank': i,
                    'score': round(match.score, 3),
                    'id': match.id,
                    'text': match.metadata.get('text', ''),
                    'source': match.metadata.get('source', 'Unknown'),
                    'title': match.metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∏'),
                    'is_relevant': match.score > 0.73
                }
                all_results.append(result_info)
                context_parts.append(match.metadata.get('text', ''))
                sources.append({
                    'id': match.id,
                    'score': match.score,
                    'source': match.metadata.get('source', 'Unknown'),
                    'title': match.metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∏')
                })
                total_score += match.score

            avg_score = total_score / len(sources) if sources else 0

            return {
                'success': len(context_parts) > 0,
                'context': '\n\n'.join(context_parts),
                'score': avg_score,
                'sources': sources,
                'raw_results': all_results,
                'total_found': len(results.matches),
                'relevant_count': len(context_parts),
                'message': f'–ó–Ω–∞–π–¥–µ–Ω–æ {len(results.matches)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤, {len(context_parts)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö'
            }
            
        except Exception as e:
            return {
                'success': False,
                'context': '',
                'score': 0.0,
                'sources': [],
                'raw_results': [],
                'error': str(e),
                'message': f'–ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É: {str(e)}'
            }
    
    def _get_embedding(self, text: str) -> List[float]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è embedding (—Ç–æ–π –∂–µ –º–µ—Ç–æ–¥ —â–æ —ñ –≤ DocumentLoader)"""
        try:
            if self.embedding_model == "local":
                return self._get_local_embedding(text)
            else:
                response = self.openai_client.embeddings.create(
                    input=text,
                    model=self.embedding_model
                )
                return response.data[0].embedding
        except Exception as e:
            return self._get_local_embedding(text)
    
    def _get_local_embedding(self, text: str) -> List[float]:
        """–õ–æ–∫–∞–ª—å–Ω–∏–π embedding (—Ç–æ—á–Ω–æ —Ç–æ–π –∂–µ —â–æ DocumentLoader)"""
        try:            
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –º–æ–¥–µ–ª—å —è–∫—â–æ —â–µ –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞
            if not hasattr(self, '_local_model'):
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¢–£ –ñ –°–ê–ú–£ –º–æ–¥–µ–ª—å —â–æ —ñ DocumentLoader
                self._local_model = SentenceTransformer('intfloat/multilingual-e5-base')
            
            embedding = self._local_model.encode(text).tolist()
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å
            stats = self.index.describe_index_stats()
            expected_dim = stats.dimension
            
            if len(embedding) != expected_dim:                
                if len(embedding) > expected_dim:
                    embedding = embedding[:expected_dim]
                elif len(embedding) < expected_dim:
                    # –†–æ–∑—à–∏—Ä—é—î–º–æ –≤–µ–∫—Ç–æ—Ä
                    while len(embedding) < expected_dim:
                        remaining = expected_dim - len(embedding)
                        if remaining >= len(embedding):
                            embedding.extend(embedding)
                        else:
                            embedding.extend(embedding[:remaining])
                    embedding = embedding[:expected_dim]
            
            return embedding
            
        except ImportError:
            raise Exception("–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install sentence-transformers")
        except Exception as e:
            raise e
    
    def generate_answer(self, query: str, context: str, state: DialogueState) -> DialogueState:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—ó
            functions = get_functions()
            
            # –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            logging.info(f"üîç –ì–µ–Ω–µ—Ä—É—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–ª—è –∑–∞–ø–∏—Ç—É: {query[:100]}...")
            logging.info(f"üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–≤–∂–∏–Ω–∞: {len(context)} —Å–∏–º–≤–æ–ª—ñ–≤")
            
            system_prompt = f"""–¢–∏ –∫–æ—Ä–∏—Å–Ω–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

–ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø –ê–ù–ê–õ–Ü–ó–£:
{context}

–Ü–ù–°–¢–†–£–ö–¶–Ü–á:
1. –Ø–∫—â–æ –∑–∞–ø–∏—Ç —Å—Ç–æ—Å—É—î—Ç—å—Å—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∑–∞–≤–¥–∞–Ω–Ω—è–º–∏ Redmine (–Ω–æ–º–µ—Ä–∏ –∑–∞–≤–¥–∞–Ω—å, —Å—Ç–∞—Ç—É—Å–∏, –≥–æ–¥–∏–Ω–∏), –≤–∏–∫–ª–∏–∫–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é
2. –Ø–∫—â–æ –∑–∞–ø–∏—Ç –ø—Ä–æ –∑–∞–≥–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
3. –Ø–∫—â–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –Ω–µ–º–∞—î –ø–æ—Ç—Ä—ñ–±–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, —Å–∫–∞–∂–∏ —â–æ –Ω–µ –∑–Ω–∞—î—à
4. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω–æ —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ
5. –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É
6. –Ø–∫—â–æ —î —Ñ—É–Ω–∫—Ü—ñ—ó, —è–∫—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–∫–ª–∏–∫–∞—Ç–∏, –≤–∫–∞–∂–∏ —ó—Ö —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"""

            # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
            messages = state.messages.copy() if state.messages else []
            messages.append({
                "role": "system", 
                "content": system_prompt
            })
            messages.append({
                "role": "user",
                "content": query
            })
            
            logging.info(f"üöÄ –í—ñ–¥–ø—Ä–∞–≤–ª—è—é –∑–∞–ø–∏—Ç –¥–æ OpenAI (model: gpt-4)")
            
            # –í–∏–∫–ª–∏–∫–∞—î–º–æ OpenAI API
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-nano",  # –ó–º—ñ–Ω–µ–Ω–æ –Ω–∞ –±—ñ–ª—å—à —Å—Ç–∞–±—ñ–ª—å–Ω—É –º–æ–¥–µ–ª—å
                messages=messages,
                temperature=0.3,
                max_tokens=1500,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –ª—ñ–º—ñ—Ç
                functions=functions,
                function_call="auto"
            )
            
            message = response.choices[0].message
            logging.info(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ OpenAI")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î function call
            if message.function_call:
                function_name = message.function_call.name
                try:
                    function_args = json.loads(message.function_call.arguments)
                    logging.info(f"üîß –í–∏–∫–ª–∏–∫–∞–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è: {function_name} –∑ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏: {function_args}")
                    
                    # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞–Ω
                    state.user_input = query
                    state.user_id = str(os.getenv("REDMINE_USER_ID", "1"))  # –ì–∞—Ä–∞–Ω—Ç—É—î–º–æ —Å—Ç—Ä–æ–∫—É
                    state.intent = function_name
                    state.function_calls = [{
                        "name": function_name,
                        "arguments": function_args
                    }]
                    state.current_node = function_name
                    
                    logging.info(f"üìù –°—Ç–∞–Ω –æ–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è function call")
                    
                except json.JSONDecodeError as je:
                    logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É function arguments: {je}")
                    state.context = f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ñ—É–Ω–∫—Ü—ñ—ó: {function_name}. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –∞—Ä–≥—É–º–µ–Ω—Ç–∏."
                
            else:
                # –ó–≤–∏—á–∞–π–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±–µ–∑ function call
                answer = message.content
                if answer:
                    state.context = answer
                    logging.info(f"‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {answer[:100]}...")
                else:
                    state.context = "‚ùå OpenAI –ø–æ–≤–µ—Ä–Ω—É–≤ –ø–æ—Ä–æ–∂–Ω—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å"
                    logging.warning("‚ö†Ô∏è OpenAI –ø–æ–≤–µ—Ä–Ω—É–≤ –ø–æ—Ä–æ–∂–Ω—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å")
        
        except Exception as e:
            # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏
            logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ generate_answer: {e}", exc_info=True)
            
            # –†—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –ø–æ–º–∏–ª–æ–∫
            if "rate limit" in str(e).lower():
                error_msg = "‚ùå –ü–µ—Ä–µ–≤–∏—â–µ–Ω–æ –ª—ñ–º—ñ—Ç –∑–∞–ø–∏—Ç—ñ–≤ –¥–æ OpenAI. –°–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ —Ö–≤–∏–ª–∏–Ω—É."
            elif "api key" in str(e).lower():
                error_msg = "‚ùå –ü—Ä–æ–±–ª–µ–º–∞ –∑ API –∫–ª—é—á–µ–º OpenAI. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è."
            elif "timeout" in str(e).lower():
                error_msg = "‚ùå –¢–∞–π–º-–∞—É—Ç –∑–∞–ø–∏—Ç—É –¥–æ OpenAI. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑."
            else:
                error_msg = f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {str(e)}"
            
            state.context = error_msg
        return state
    
    def format_search_results(self, rag_result: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è"""
        if not rag_result.get('raw_results'):
            return "üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É –Ω–µ–º–∞—î"
        
        output = []
        output.append(f"üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É –≤ –±–∞–∑—ñ –∑–Ω–∞–Ω—å:**")
        output.append(f"üìä {rag_result.get('message', '')}")
        output.append("")
        
        for result in rag_result['raw_results']:
            relevance_icon = "‚úÖ" if result['is_relevant'] else "‚ö†Ô∏è"
            score_percent = f"{result['score']*100:.1f}%"
            
            output.append(f"{relevance_icon} **#{result['rank']} - {result['title']}** ({score_percent})")
            
            # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 150 —Å–∏–º–≤–æ–ª—ñ–≤ —Ç–µ–∫—Å—Ç—É
            text_preview = result['text'][:150] + "..." if len(result['text']) > 150 else result['text']
            output.append(f"üìÑ {text_preview}")
            output.append(f"üè∑Ô∏è –î–∂–µ—Ä–µ–ª–æ: {result['source']}")
            output.append("")
        
        return "\n".join(output)